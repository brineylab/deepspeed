# Model Config
vocab_size: 25
num_attention_heads: 16
num_hidden_layers: 24
hidden_size: 1024
intermediate_size: 4096
max_len: 512
max_position_embeddings: 514
type_vocab_size: 2 # roberta architectures only

# Tokenizer
tokenizer_path: "./tokenizer/"
padding: "max_length"
max_length: 512
truncation: True
add_special_tokens: True
return_special_tokens_mask: True

# Datasets
train_file: "train.parquet"
validation_file: "eval.parquet"
sequence_column: "text"
file_type: "parquet"

# Collator
mlm: True
mlm_probability: 0.15

# Training Arguments
run_name: "BALM_ds-test"
fp16: True
seed: 42
batch_size: 32
gradient_accumulation_steps: 1
logging_steps: 100
evaluation_strategy: "steps"
eval_steps: 25000

warmup_steps: 30000
max_steps: 500000

peak_learning_rate: 0.0004
weight_decay: 0.01
adam_epsilon: 0.000001
adam_beta1: 0.9
adam_beta2: 0.98

# Logging & Outputs
# logging and output directories are set in the training script
overwrite_output_dir: True
report_to: "wandb"
wandb_project: "huggingface" # update this with your own project!
logging_first_step: True