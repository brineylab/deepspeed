# Model Config
vocab_size: 26
pad_token_id: 21
mask_token_id: 22
num_attention_heads: 20
num_hidden_layers: 32
hidden_size: 960
intermediate_size: 3840
max_position_embeddings: 322
position_embedding_type: "rotary"

# Tokenizer
tokenizer_path: "vocab.txt"
padding: "max_length"
max_length: 320
truncation: True
return_special_tokens_mask: False
separator_token: "<cls>"

# Datasets
train_file: "train.csv"
validation_file: "eval.csv"
file_type: "csv"

# Collator
mlm: True
mlm_probability: 0.15

# Training Arguments
run_name: "BALM-paired-ESM_${date}"
fp16: True
seed: 42
batch_size: 32
gradient_accumulation_steps: 1
logging_steps: 100
evaluation_strategy: "steps"
eval_steps: 25000
save_steps: 50000

warmup_steps: 30000
max_steps: 500000

peak_learning_rate: 0.0004
weight_decay: 0.01
adam_epsilon: 0.000001
adam_beta1: 0.9
adam_beta2: 0.98

# Logging & Outputs
# logging and output directories are set in the training script
overwrite_output_dir: True
report_to: "wandb"
wandb_project: "huggingface" # update this with your own project!
logging_first_step: True