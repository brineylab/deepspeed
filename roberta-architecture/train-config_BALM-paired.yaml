# Model Config
vocab_size: 25
num_attention_heads: 16
num_hidden_layers: 24
hidden_size: 1024
intermediate_size: 4096
max_len: 512
max_position_embeddings: 514
type_vocab_size: 2 # roberta architectures only

# Tokenizer
tokenizer_path: "/home/jovyan/shared/Sarah/BALM-paper/pre-training/BALM/tokenizer/"
padding: "max_length"
max_length: 512
truncation: True
add_special_tokens: True
return_special_tokens_mask: True

# Datasets
train_file: "/home/jovyan/shared/Sarah/training-data/jaffe_lc-coherence/paired/LC-coherence_90-5-5/cls-seperator/train.parquet"
validation_file: "/home/jovyan/shared/Sarah/training-data/jaffe_lc-coherence/paired/LC-coherence_90-5-5/cls-seperator/eval.parquet"
sequence_column: "text" # to enable use of differently formatted datasets
file_type: "parquet"

# Collator
mlm: True
mlm_probability: 0.15

# Training Arguments
run_name: "BALM_ds-test"
fp16: True
seed: 42
batch_size: 32
gradient_accumulation_steps: 1
logging_steps: 100
evaluation_strategy: "steps"
eval_steps: 25000

warmup_steps: 30000
max_steps: 500000

peak_learning_rate: 0.0004
weight_decay: 0.01
adam_epsilon: 0.000001
adam_beta1: 0.9
adam_beta2: 0.98

overwrite_output_dir: True
output_dir: "./checkpoints/{run_name}"
report_to: "wandb"
wandb_project: "sarah_testing"
wandb_job_type: "deepspeed-stages"
logging_first_step: True
logging_dir: "./wandb/{run_name}"
deepspeed: "deepspeed_config.json"